# vLLM Server Requirements

# Core vLLM and inference
vllm==0.2.7
transformers>=4.36.0
torch>=2.1.0
tokenizers>=0.15.0
sentencepiece>=0.1.99

# Web framework
fastapi==0.104.1
uvicorn[standard]==0.24.0
pydantic==2.5.0

# Hugging Face integration
huggingface-hub>=0.19.0
accelerate>=0.24.0
datasets>=2.14.0

# CUDA and GPU support
nvidia-ml-py3>=7.352.0

# Async utilities
aiofiles>=23.2.0
asyncio-mqtt>=0.11.0

# Monitoring and logging
prometheus-client>=0.19.0
structlog>=23.2.0

# Utilities
click>=8.1.7
python-multipart>=0.0.6
python-jose[cryptography]>=3.3.0
python-dotenv>=1.0.0
pyyaml>=6.0.1
requests>=2.31.0

# Development and testing (optional)
pytest>=7.4.3
pytest-asyncio>=0.21.1
httpx>=0.25.2