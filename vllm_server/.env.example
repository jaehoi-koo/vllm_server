# vLLM Server Configuration
# Copy this file to .env and configure your settings

# Server Configuration
VLLM_HOST=0.0.0.0
VLLM_PORT=8001
VLLM_WORKERS=1

# Model Configuration
# Set the path to your base model (local path or Hugging Face repo ID)
VLLM_MODEL_PATH=microsoft/DialoGPT-medium
VLLM_TENSOR_PARALLEL_SIZE=1
VLLM_GPU_MEMORY_UTILIZATION=0.8
VLLM_TRUST_REMOTE_CODE=true

# LoRA Configuration  
VLLM_ENABLE_LORA=true
VLLM_MAX_LORAS=4
VLLM_MAX_LORA_RANK=64
VLLM_LORA_CACHE_DIR=./lora_cache
VLLM_MAX_CACHE_SIZE_GB=50.0

# Generation Defaults
VLLM_DEFAULT_TEMPERATURE=0.7
VLLM_DEFAULT_MAX_TOKENS=512
VLLM_DEFAULT_TOP_P=0.9
VLLM_DEFAULT_TOP_K=-1

# Hugging Face Configuration
# Get your token from: https://huggingface.co/settings/tokens
HF_TOKEN=your_huggingface_token_here
HF_HOME=./models/.cache

# Logging
VLLM_LOG_LEVEL=INFO
VLLM_LOG_DIR=./logs

# Security
VLLM_ENABLE_CORS=true
VLLM_CORS_ORIGINS=["*"]

# Monitoring
VLLM_ENABLE_METRICS=true
VLLM_METRICS_PORT=9090

# Performance
VLLM_MAX_CONCURRENT_REQUESTS=100
VLLM_REQUEST_TIMEOUT=300.0

# GPU Configuration
CUDA_VISIBLE_DEVICES=0